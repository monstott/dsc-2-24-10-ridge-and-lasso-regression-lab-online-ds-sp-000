{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll practice your knowledge on Ridge and Lasso regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Lasso and ridge regression in Python\n",
    "- Compare Lasso and Ridge with standard regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Prices Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at yet another house pricing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('Housing_Prices/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      "Id               1460 non-null int64\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1201 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "Alley            91 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1452 non-null object\n",
      "MasVnrArea       1452 non-null float64\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1423 non-null object\n",
      "BsmtCond         1423 non-null object\n",
      "BsmtExposure     1422 non-null object\n",
      "BsmtFinType1     1423 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1422 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      770 non-null object\n",
      "GarageType       1379 non-null object\n",
      "GarageYrBlt      1379 non-null float64\n",
      "GarageFinish     1379 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1379 non-null object\n",
      "GarageCond       1379 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "PoolQC           7 non-null object\n",
      "Fence            281 non-null object\n",
      "MiscFeature      54 non-null object\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a first selection of the data by removing some of the data with `dtype = object`, this way our first model only contains **continuous features**\n",
    "\n",
    "Make sure to remove the SalesPrice column from the predictors (which you store in `X`), then replace missing inputs by the median per feature.\n",
    "\n",
    "Store the target in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# remove \"object\"-type features and SalesPrice from `X`\n",
    "features = [col for col in df.columns if df[col].dtype in [np.float64, np.int64] and col!='SalePrice']\n",
    "X = df[features]\n",
    "\n",
    "# Impute null values\n",
    "for col in X:\n",
    "    feature_med = X[col].median()\n",
    "    X[col].fillna(value = feature_med, inplace = True)\n",
    "\n",
    "# Create y\n",
    "y = df.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the information of `X` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 37 columns):\n",
      "Id               1460 non-null int64\n",
      "MSSubClass       1460 non-null int64\n",
      "LotFrontage      1460 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "MasVnrArea       1460 non-null float64\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Fireplaces       1460 non-null int64\n",
      "GarageYrBlt      1460 non-null float64\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "dtypes: float64(3), int64(34)\n",
      "memory usage: 422.1 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use this data to perform a first naive linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the R squared and the MSE for both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 =  0.8263424255293087\n",
      "Testing R2 =  0.755677831544955\n",
      "Training MSE =  1064028890.2885737\n",
      "Testing MSE =  1671890629.2838702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# Fit the model and print R2 and MSE for train and test\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "print('Training R2 = ', linreg.score(X_train, y_train))\n",
    "print('Testing R2 = ', linreg.score(X_test, y_test))\n",
    "print('Training MSE = ', mean_squared_error(y_train, linreg.predict(X_train)))\n",
    "print('Testing MSE = ', mean_squared_error(y_test, linreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't normalized our data, let's create a new model that uses `preprocessing.scale` to scale our predictors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Scale the data and perform train test split\n",
    "X_scale = preprocessing.scale(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same linear regression on this data and print out R-squared and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 =  0.8086637169775084\n",
      "Testing R2 =  0.8056307714161693\n",
      "Training MSE =  1259808184.8739574\n",
      "Testing MSE =  1057443804.4265274\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "linreg_norm = LinearRegression()\n",
    "linreg_norm.fit(X_train, y_train)\n",
    "print('Training R2 = ', linreg_norm.score(X_train, y_train))\n",
    "print('Testing R2 = ', linreg_norm.score(X_test, y_test))\n",
    "print('Training MSE = ', mean_squared_error(y_train, linreg_norm.predict(X_train)))\n",
    "print('Testing MSE = ', mean_squared_error(y_test, linreg_norm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't included dummy variables so far: let's use our \"object\" variables again and create dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape without dummies:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1460, 43)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create X_cat which contains only the categorical variables\n",
    "features_cat = [col for col in df.columns if df[col].dtype in [np.object]]\n",
    "X_cat = df[features_cat]\n",
    "print('Shape without dummies:')\n",
    "np.shape(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape with dummies:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1460, 252)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make dummies\n",
    "X_cat = pd.get_dummies(X_cat)\n",
    "print('Shape with dummies:')\n",
    "np.shape(X_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge `x_cat` together with our scaled `X` so you have one big predictor dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X_merge = pd.concat([pd.DataFrame(X_scale), X_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same linear regression on this data and print out R-squared and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 =  0.94074241359729\n",
      "Testing R2 =  -5.0441900304572774e+17\n",
      "Training MSE =  369103563.91872144\n",
      "Testing MSE =  3.297648324586507e+27\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_merge, y)\n",
    "linreg_merge = LinearRegression()\n",
    "linreg_merge.fit(X_train, y_train)\n",
    "print('Training R2 = ', linreg_merge.score(X_train, y_train))\n",
    "print('Testing R2 = ', linreg_merge.score(X_test, y_test))\n",
    "print('Training MSE = ', mean_squared_error(y_train, linreg_merge.predict(X_train)))\n",
    "print('Testing MSE = ', mean_squared_error(y_test, linreg_merge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the severe overfitting above; our training R squared is quite high, but the testing R squared is negative! Our predictions are far far off. Similarly, the scale of the Testing MSE is orders of magnitude higher then that of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Ridge and Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all the data (normalized features and dummy categorical variables) and perform Lasso and Ridge regression for both! Each time, look at R-squared and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With default parameter (alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 =  0.9360740134925611\n",
      "Testing R2 = 0.8870946240413695\n",
      "Training MSE =  396594383.52537334\n",
      "Testing MSE =  741099013.0957825\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Lasso() \n",
    "lasso.fit(X_train, y_train)\n",
    "print('Training R2 = ', lasso.score(X_train, y_train))\n",
    "print('Testing R2 =', lasso.score(X_test, y_test))\n",
    "print('Training MSE = ', mean_squared_error(y_train, lasso.predict(X_train)))\n",
    "print('Testing MSE = ', mean_squared_error(y_test, lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a higher regularization parameter (alpha = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 =  0.9342824984286946\n",
      "Testing R2 =  0.8923618830441477\n",
      "Training MSE =  407708874.6916214\n",
      "Testing MSE =  706525278.9796225\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "lasso = Lasso(alpha=10)\n",
    "lasso.fit(X_train, y_train)\n",
    "print('Training R2 = ', lasso.score(X_train, y_train))\n",
    "print('Testing R2 = ', lasso.score(X_test, y_test))\n",
    "print('Training MSE = ', mean_squared_error(y_train, lasso.predict(X_train)))\n",
    "print('Testing MSE = ', mean_squared_error(y_test, lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With default parameter (alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 = 0.9233007663063825\n",
      "Testing R2 =  0.8728342514049975\n",
      "Training MSE =  475839122.1705914\n",
      "Testing MSE =  834702599.2621821\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "print('Training R2 =', ridge.score(X_train, y_train))\n",
    "print('Testing R2 = ', ridge.score(X_test, y_test))\n",
    "print('Training MSE = ', mean_squared_error(y_train, ridge.predict(X_train)))\n",
    "print('Testing MSE = ', mean_squared_error(y_test, ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With default parameter (alpha = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 =  0.8998090139851723\n",
      "Testing R2 =  0.87599624557936\n",
      "Training MSE =  621581058.0995269\n",
      "Testing MSE =  813947602.0608683\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "ridge = Ridge(alpha = 10)\n",
    "ridge.fit(X_train, y_train)\n",
    "print('Training R2 = ', ridge.score(X_train, y_train))\n",
    "print('Testing R2 = ', ridge.score(X_test, y_test))\n",
    "print('Training MSE = ', mean_squared_error(y_train, ridge.predict(X_train)))\n",
    "print('Testing MSE = ', mean_squared_error(y_test, ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the metrics, what are your main conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions here\n",
    "\n",
    "First Naive Linear Regression\n",
    "Training R2 =  0.83\n",
    "Testing R2 =  0.76\n",
    "Training MSE =  1,064,028,890\n",
    "Testing MSE =  1,671,890,629\n",
    "\n",
    "Normalized Linear Regression\n",
    "Training R2 =  0.81\n",
    "Testing R2 =  0.81\n",
    "Training MSE =  1,259,808,184\n",
    "Testing MSE =  1,057,443,804\n",
    "\n",
    "Normalized + Continuous Dummies Linear Regression\n",
    "Training R2 =  0.94\n",
    "Testing R2 =  -2.30e+19\n",
    "Training MSE =  383,232,328\n",
    "Testing MSE =  1.51e+29\n",
    "\n",
    "Of the three linear regressions, the Normalized Linear Regression has the highest Testing R2 and lowest Testing MSE. The addition of Continuous Dummies to this increases the Training R2 but is catastrophic to Testing R2 and MSE.\n",
    "\n",
    "Lasso (alpha=1) Regression\n",
    "Training R2 =  0.94\n",
    "Testing R2 = 0.89\n",
    "Training MSE =  396,594,383\n",
    "Testing MSE =  741,099,013\n",
    "\n",
    "Lasso (alpha=10) Regression\n",
    "Training R2 =  0.93\n",
    "Testing R2 =  0.89\n",
    "Training MSE =  407,708,874\n",
    "Testing MSE =  706,525,278\n",
    "\n",
    "The two Lasso regressions are nearly identical, with the alpha = 10 version having slightly lower Testing MSE.\n",
    "\n",
    "Ridge (alpha=1) Regression\n",
    "Training R2 = 0.92\n",
    "Testing R2 =  0.87\n",
    "Training MSE =  475,839,122\n",
    "Testing MSE =  834,702,599\n",
    "\n",
    "Ridge (alpha=10) Regression\n",
    "Training R2 =  0.90\n",
    "Testing R2 =  0.88\n",
    "Training MSE =  621,581,058\n",
    "Testing MSE =  813,947,602\n",
    "\n",
    "The two Lasso regressions are nearly identical, with the alpha = 10 version having slightly lower Testing MSE. \n",
    "\n",
    "Overall, Lasso (alpha=10) Regression has the lowest Testing MSE. The highest Testing R2 results from either Lasso version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare number of parameter estimates that are (very close to) 0 for Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# number of Ridge params almost zero\n",
    "print(sum(abs(ridge.coef_) < 10**(-10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "# number of Lasso params almost zero\n",
    "print(sum(abs(lasso.coef_) < 10**(-10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the total length of the parameter space and draw conclusions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "len(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03806228373702422"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(abs(ridge.coef_) < 10**(-10))/ len(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2491349480968858"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(abs(lasso.coef_) < 10**(-10)) / len(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of parameter estimates that are very close to 0 are: for Ridge, 11 (3.8%); for Lasso, 72 (24.9%). \n",
    "# The total length of the parameter space is 289. \n",
    "# Lasso does a better job of reducing the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You now know how to perform Lasso and Ridge regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
